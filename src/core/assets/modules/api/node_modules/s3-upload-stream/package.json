{
  "_args": [
    [
      {
        "raw": "s3-upload-stream@~1.0.7",
        "scope": null,
        "escapedName": "s3-upload-stream",
        "name": "s3-upload-stream",
        "rawSpec": "~1.0.7",
        "spec": ">=1.0.7 <1.1.0",
        "type": "range"
      },
      "/Users/vt/Desktop/new/node_modules/pkgcloud"
    ]
  ],
  "_from": "s3-upload-stream@>=1.0.7 <1.1.0",
  "_id": "s3-upload-stream@1.0.7",
  "_inCache": true,
  "_location": "/s3-upload-stream",
  "_nodeVersion": "0.10.33",
  "_npmUser": {
    "name": "nathanpeck",
    "email": "nathan.peck1@gmail.com"
  },
  "_npmVersion": "2.1.10",
  "_phantomChildren": {},
  "_requested": {
    "raw": "s3-upload-stream@~1.0.7",
    "scope": null,
    "escapedName": "s3-upload-stream",
    "name": "s3-upload-stream",
    "rawSpec": "~1.0.7",
    "spec": ">=1.0.7 <1.1.0",
    "type": "range"
  },
  "_requiredBy": [
    "/pkgcloud"
  ],
  "_resolved": "https://registry.npmjs.org/s3-upload-stream/-/s3-upload-stream-1.0.7.tgz",
  "_shasum": "e3f80253141c569f105a62aa50ca9b45760e481d",
  "_shrinkwrap": null,
  "_spec": "s3-upload-stream@~1.0.7",
  "_where": "/Users/vt/Desktop/new/node_modules/pkgcloud",
  "author": {
    "name": "Nathan Peck",
    "email": "nathan@storydesk.com"
  },
  "bugs": {
    "url": "https://github.com/nathanpeck/s3-upload-stream/issues"
  },
  "dependencies": {},
  "description": "Writeable stream for uploading content of unknown size to S3 via the multipart API.",
  "devDependencies": {
    "aws-sdk": "2.0.17",
    "chai": "1.8.1",
    "mocha": "1.17.1"
  },
  "directories": {},
  "dist": {
    "shasum": "e3f80253141c569f105a62aa50ca9b45760e481d",
    "tarball": "https://registry.npmjs.org/s3-upload-stream/-/s3-upload-stream-1.0.7.tgz"
  },
  "engines": {
    "node": ">=0.10.x"
  },
  "gitHead": "d410c27148b30c14de2e97d61d42971b8c37f5af",
  "homepage": "https://github.com/nathanpeck/s3-upload-stream#readme",
  "keywords": [
    "aws",
    "s3",
    "upload",
    "pipe",
    "stream"
  ],
  "license": "MIT",
  "main": "./lib/s3-upload-stream.js",
  "maintainers": [
    {
      "name": "nathanpeck",
      "email": "nathan@storydesk.com"
    }
  ],
  "name": "s3-upload-stream",
  "optionalDependencies": {},
  "peerDependencies": {
    "aws-sdk": "2.x"
  },
  "readme": "## s3-upload-stream [![Build Status](https://travis-ci.org/nathanpeck/s3-upload-stream.svg)](https://travis-ci.org/nathanpeck/s3-upload-stream)\n\nA pipeable write stream which uploads to Amazon S3 using the multipart file upload API.\n\n[![NPM](https://nodei.co/npm/s3-upload-stream.png?downloads=true)](https://www.npmjs.org/package/s3-upload-stream)\n\n### Changelog\n\n#### 1.0.6 (2014-10-20)\n\nRemoving global state, and adding pause and resume functionality.\n\n[Historical Changelogs](CHANGELOG.md)\n\n### Why use this stream?\n\n* This upload stream does not require you to know the length of your content prior to beginning uploading. Many other popular S3 wrappers such as [Knox](https://github.com/LearnBoost/knox) also allow you to upload streams to S3, but they require you to specify the content length. This is not always feasible.\n* By piping content to S3 via the multipart file upload API you can keep memory usage low even when operating on a stream that is GB in size. Many other libraries actually store the entire stream in memory and then upload it in one piece. This stream avoids high memory usage by flushing the stream to S3 in 5 MB parts such that it should only ever store 5 MB of the stream data at a time.\n* This package is designed to use the official Amazon SDK for Node.js, helping keep it small and efficient. For maximum flexibility you pass in the aws-sdk client yourself, allowing you to use a uniform version of AWS SDK throughout your code base.\n* You can provide options for the upload call directly to do things like set server side encryption, reduced redundancy storage, or access level on the object, which some other similar streams are lacking.\n* Emits \"part\" events which expose the amount of incoming data received by the writable stream versus the amount of data that has been uploaded via the multipart API so far, allowing you to create a progress bar if that is a requirement.\n* Support for pausing and later resuming in progress multipart uploads.\n\n### Limits\n\n* The multipart upload API does not accept parts less than 5 MB in size. So although this stream emits \"part\" events which can be used to show progress, the progress is not very granular, as the events are only per part. By default this means that you will receive an event each 5 MB.\n* The Amazon SDK has a limit of 10,000 parts when doing a mulitpart upload. Since the part size is currently set to 5 MB this means that your stream will fail to upload if it contains more than 50 GB of data. This can be solved by using the 'stream.maxPartSize()' method of the writable stream to set the max size of an upload part, as documented below. By increasing this value you should be able to save streams that are many TB in size.\n\n## Example\n\n```js\nvar AWS      = require('aws-sdk'),\n    zlib     = require('zlib'),\n    fs       = require('fs');\n    s3Stream = require('s3-upload-stream')(new AWS.S3()),\n\n// Set the client to be used for the upload.\nAWS.config.loadFromPath('./config.json');\n\n// Create the streams\nvar read = fs.createReadStream('/path/to/a/file');\nvar compress = zlib.createGzip();\nvar upload = s3Stream.upload({\n  \"Bucket\": \"bucket-name\",\n  \"Key\": \"key-name\"\n});\n\n// Optional configuration\nupload.maxPartSize(20971520); // 20 MB\nupload.concurrentParts(5);\n\n// Handle errors.\nupload.on('error', function (error) {\n  console.log(error);\n});\n\n/* Handle progress. Example details object:\n   { ETag: '\"f9ef956c83756a80ad62f54ae5e7d34b\"',\n     PartNumber: 5,\n     receivedSize: 29671068,\n     uploadedSize: 29671068 }\n*/\nupload.on('part', function (details) {\n  console.log(details);\n});\n\n/* Handle upload completion. Example details object:\n   { Location: 'https://bucketName.s3.amazonaws.com/filename.ext',\n     Bucket: 'bucketName',\n     Key: 'filename.ext',\n     ETag: '\"bf2acbedf84207d696c8da7dbb205b9f-5\"' }\n*/\nupload.on('uploaded', function (details) {\n  console.log(details);\n});\n\n// Pipe the incoming filestream through compression, and up to S3.\nread.pipe(compress).pipe(upload);\n```\n\n## Usage\n\nBefore uploading you must configure the S3 client for s3-upload-stream to use. Please note that this module has only been tested with AWS SDK 2.0 and greater.\n\nThis module does not include the AWS SDK itself. Rather you must require the AWS SDK in your own application code, instantiate an S3 client and then supply it to s3-upload-stream.\n\nThe main advantage of this is that rather than being stuck with a set version of the AWS SDK that ships with s3-upload-stream you can ensure that s3-upload-stream is using whichever verison of the SDK you want.\n\nWhen setting up the S3 client the recommended approach for credential management is to [set your AWS API keys using environment variables](http://docs.aws.amazon.com/AWSJavaScriptSDK/guide/node-configuring.html) or [AMI roles](http://docs.aws.amazon.com/IAM/latest/UserGuide/WorkingWithRoles.html).\n\nIf you are following this approach then you can configure the S3 client very simply:\n\n```js\nvar AWS      = require('aws-sdk'),\n    s3Stream = require('../lib/s3-upload-stream.js')(new AWS.S3());\n```\n\nHowever, some environments may require you to keep your credentials in a file, or hardcoded. In that case you can use the following form:\n\n```js\nvar AWS      = require('aws-sdk');\n\n// Make sure AWS credentials are loaded using one of the following techniques\nAWS.config.loadFromPath('./config.json');\nAWS.config.update({accessKeyId: 'akid', secretAccessKey: 'secret'});\n\n// Create a stream client.\nvar s3Stream = require('../lib/s3-upload-stream.js')(new AWS.S3());\n```\n\n### client.upload(destination)\n\nCreate an upload stream that will upload to the specified destination. The upload stream is returned immeadiately.\n\nThe destination details is an object in which you can specify many different [destination properties enumerated in the AWS S3 documentation](http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3.html#createMultipartUpload-property).\n\n__Example:__\n\n```js\nvar AWS      = require('aws-sdk'),\n    s3Stream = require('../lib/s3-upload-stream.js')(new AWS.S3());\n\nvar read = fs.createReadStream('/path/to/a/file');\nvar upload = s3Stream.upload({\n  Bucket: \"bucket-name\",\n  Key: \"key-name\",\n  ACL: \"public-read\",\n  StorageClass: \"REDUCED_REDUNDANCY\",\n  ContentType: \"binary/octet-stream\"\n});\n\nread.pipe(upload);\n```\n\n### client.upload(destination, [session])\n\nResume an incomplete multipart upload from a previous session by providing a `session` object with an upload ID, and ETag and numbers for each part. `destination` details is as above.\n\n__Example:__\n\n```js\nvar AWS      = require('aws-sdk'),\n    s3Stream = require('../lib/s3-upload-stream.js')(new AWS.S3());\n\nvar read = fs.createReadStream('/path/to/a/file');\nvar upload = s3Stream.upload(\n  {\n    Bucket: \"bucket-name\",\n    Key: \"key-name\",\n    ACL: \"public-read\",\n    StorageClass: \"REDUCED_REDUNDANCY\",\n    ContentType: \"binary/octet-stream\"\n  },\n  {\n    UploadId: \"f1j2b47238f12984f71b2o8347f12\",\n    Parts: [\n      {\n        ETag: \"3k2j3h45t9v8aydgajsda\",\n        PartNumber: 1\n      },\n      {\n        Etag: \"kjgsdfg876sd8fgk3j44t\",\n        PartNumber: 2\n      }\n    ]\n  }\n);\n\nread.pipe(upload);\n```\n\n## Stream Methods\n\nThe following methods can be called on the stream returned by from `client.upload()`\n\n### stream.pause()\n\nPause an active multipart upload stream.\n\nCalling `pause()` will immediately:\n\n* stop accepting data from an input stream,\n* stop submitting new parts for upload, and\n* emit a `pausing` event with the number of parts that are still mid-upload.\n\nWhen mid-upload parts are finished, a `paused` event will fire, including an object with `UploadId` and `Parts` data that can be used to resume an upload in a later session.\n\n### stream.resume()\n\nResume a paused multipart upload stream.\n\nCalling `resume()` will immediately:\n\n* resume accepting data from an input stream,\n* resume submitting new parts for upload, and\n* echo a `resume` event back to any listeners.\n\nIt is safe to call `resume()` at any time after `pause()`. If the stream is between `pausing` and `paused`, then `resume()` will resume data flow and the `paused` event will not be fired.\n\n### stream.maxPartSize(sizeInBytes)\n\nUsed to adjust the maximum amount of stream data that will be buffered in memory prior to flushing. The lowest possible value, and default value, is 5 MB. It is not possible to set this value any lower than 5 MB due to Amazon S3 restrictions, but there is no hard upper limit. The higher the value you choose the more stream data will be buffered in memory before flushing to S3.\n\nThe main reason for setting this to a higher value instead of using the default is if you have a stream with more than 50 GB of data, and therefore need larger part sizes in order to flush the entire stream while also staying within Amazon's upper limit of 10,000 parts for the multipart upload API.\n\n```js\nvar AWS      = require('aws-sdk'),\n    s3Stream = require('../lib/s3-upload-stream.js')(new AWS.S3());\n\nvar read = fs.createReadStream('/path/to/a/file');\nvar upload = s3Stream.upload({\n  \"Bucket\": \"bucket-name\",\n  \"Key\": \"key-name\"\n});\n\nupload.maxPartSize(20971520); // 20 MB\n\nread.pipe(upload);\n```\n\n### stream.concurrentParts(numberOfParts)\n\nUsed to adjust the number of parts that are concurrently uploaded to S3. By default this is just one at a time, to keep memory usage low and allow the upstream to deal with backpressure. However, in some cases you may wish to drain the stream that you are piping in quickly, and then issue concurrent upload requests to upload multiple parts.\n\nKeep in mind that total memory usage will be at least `maxPartSize` * `concurrentParts` as each concurrent part will be `maxPartSize` large, so it is not recommended that you set both `maxPartSize` and `concurrentParts` to high values, or your process will be buffering large amounts of data in its memory.\n\n```js\nvar AWS      = require('aws-sdk'),\n    s3Stream = require('../lib/s3-upload-stream.js')(new AWS.S3());\n\nvar read = fs.createReadStream('/path/to/a/file');\nvar upload = s3Stream.upload({\n  \"Bucket\": \"bucket-name\",\n  \"Key\": \"key-name\"\n});\n\nupload.concurrentParts(5);\n\nread.pipe(upload);\n```\n\n### Tuning configuration of the AWS SDK\n\nThe following configuration tuning can help prevent errors when using less reliable internet connections (such as 3G data if you are using Node.js on the Tessel) by causing the AWS SDK to detect upload timeouts and retry.\n\n```js\nvar AWS = require('aws-sdk');\nAWS.config.httpOptions = {timeout: 5000};\n```\n\n### Installation\n\n```\nnpm install s3-upload-stream\n```\n\n### Running Tests\n\n```\nnpm test\n```\n\n### License\n\n(The MIT License)\n\nCopyright (c) 2014 Nathan Peck <nathan@storydesk.com>\n\nPermission is hereby granted, free of charge, to any person obtaining\na copy of this software and associated documentation files (the\n'Software'), to deal in the Software without restriction, including\nwithout limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so, subject to\nthe following conditions:\n\nThe above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED 'AS IS', WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\nIN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\nCLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\nTORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\nSOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
  "readmeFilename": "README.md",
  "repository": {
    "type": "git",
    "url": "git://github.com/nathanpeck/s3-upload-stream.git"
  },
  "scripts": {
    "test": "mocha -R spec -s 100 ./tests/test.js"
  },
  "version": "1.0.7"
}
